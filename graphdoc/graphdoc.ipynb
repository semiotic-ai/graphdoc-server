{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello, world!\n"
     ]
    }
   ],
   "source": [
    "print(\"hello, world!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# system packages \n",
    "import os\n",
    "import json\n",
    "\n",
    "# internal packages \n",
    "\n",
    "# external packages\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "from graphql import build_schema, parse\n",
    "from jinja2 import Environment, FileSystemLoader\n",
    "\n",
    "load_dotenv()\n",
    "open_ai_api_key = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GraphQL Schema Parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# with open('../assets/schemas/opensea/opensea_original_schema.graphql', 'r') as schema_file:\n",
    "#     schema_str = schema_file.read()\n",
    "\n",
    "# # TEMP: Try to parse the schema given the errors we are seeing\n",
    "# custom_definitions = '''\n",
    "# scalar BigDecimal\n",
    "# scalar BigInt\n",
    "\n",
    "# directive @entity on OBJECT\n",
    "# directive @dailySnapshot on OBJECT\n",
    "# directive @regularPolling on OBJECT\n",
    "# directive @derivedFrom on OBJECT\n",
    "# directive @transaction on OBJECT\n",
    "# '''\n",
    "\n",
    "# full_schema_str = custom_definitions + schema_str\n",
    "# schema = build_schema(full_schema_str)\n",
    "\n",
    "# for type_name, graphql_type in schema.type_map.items():\n",
    "#     if type_name.startswith('__'):\n",
    "#         continue  # Skip introspection types\n",
    "#     print(f\"Type: {type_name}\")\n",
    "#     for field_name, field in graphql_type.fields.items():\n",
    "#         print(f\"  Field: {field_name} (type: {field.type})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LangGraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import getpass\n",
    "# import os\n",
    "\n",
    "# if \"OPENAI_API_KEY\" not in os.environ:\n",
    "#     os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter your OpenAI API key: \")\n",
    "# open_ai_api_key = os.environ[\"OPENAI_API_KEY\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from typing import Annotated\n",
    "\n",
    "# from langchain_openai import OpenAI\n",
    "# from typing_extensions import TypedDict\n",
    "\n",
    "# from langgraph.graph import StateGraph\n",
    "# from langgraph.graph.message import add_messages\n",
    "\n",
    "\n",
    "# class State(TypedDict):\n",
    "#     messages: Annotated[list, add_messages]\n",
    "\n",
    "\n",
    "# graph_builder = StateGraph(State)\n",
    "\n",
    "\n",
    "# llm = OpenAI(model=\"gpt-4\")\n",
    "\n",
    "\n",
    "# def chatbot(state: State):\n",
    "#     return {\"messages\": [llm.invoke(state[\"messages\"])]}\n",
    "\n",
    "# graph_builder.add_node(\"chatbot\", chatbot)\n",
    "# graph_builder.set_entry_point(\"chatbot\")\n",
    "# graph_builder.set_finish_point(\"chatbot\")\n",
    "# graph = graph_builder.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from IPython.display import Image, display\n",
    "\n",
    "# try:\n",
    "#     display(Image(graph.get_graph().draw_mermaid_png()))\n",
    "# except Exception:\n",
    "#     pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OpenAI Directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! I'm here and ready to help. How can I assist you today?\n"
     ]
    }
   ],
   "source": [
    "client = OpenAI(\n",
    "    api_key=os.environ.get(\"OPENAI_API_KEY\"),  # This is the default and can be omitted\n",
    ")\n",
    "\n",
    "def gpt_chat_completion(client, message, model=\"gpt-4o\"):\n",
    "    return client.chat.completions.create(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": message,\n",
    "        }\n",
    "    ],\n",
    "    model=model,\n",
    ")\n",
    "\n",
    "print(gpt_chat_completion(client, \"Hello! How are you?\").choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial Test Set \n",
    "\n",
    "### Prompts \n",
    "\n",
    "- [ ] One entity quality vs. another entity quality \n",
    "- [ ] One column quality vs. another column quality \n",
    "- [ ] One schema quality vs. another schema quality \n",
    "\n",
    "### Demonstrations \n",
    "\n",
    "- [ ] A high quality entity (description)\n",
    "- [ ] A high quality column (description)\n",
    "- [ ] A high quality schema (description)\n",
    "\n",
    "### Quality Categories\n",
    "\n",
    "- [ ] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"tests/assets/entity_comparison_assets.json\", \"r\") as f:\n",
    "    entity_comparison_assets = json.load(f)\n",
    "\n",
    "gold_entity_comparison = \"\".join(entity_comparison_assets[\"gold\"][\"prompt\"])\n",
    "four_entity_comparison = \"\".join(entity_comparison_assets[\"four\"][\"prompt\"])\n",
    "three_entity_comparison = \"\".join(entity_comparison_assets[\"three\"][\"prompt\"])\n",
    "two_entity_comparison = \"\".join(entity_comparison_assets[\"two\"][\"prompt\"])\n",
    "one_entity_comparison = \"\".join(entity_comparison_assets[\"one\"][\"prompt\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Template 'entity_comparison_prompt.txt'>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expecting output: 3 \n",
      "\n",
      "{'reasoning': \"First, we must compare the EVALUATION DOCUMENTATION with the GOLD DOCUMENTATION. Upon careful examination, both documentations are identical in their format and content. Each column in both documentations contains a full English sentence that clearly describes the meaning of the column, eliminating ambiguity. For example, all entries specify the context when necessary, such as 'The marketplace that this snapshot belongs to' rather than a less informative 'Market.' Also, they include concise yet comprehensive descriptions, such as explaining that a column represents 'Cumulative trade volume (in ETH).' The queries are syntactically correct, using proper capitalization and punctuation. Therefore, the provided documentation matches the GOLD documentation criteria and lacks any missing information, misleading descriptions, or grammar errors.\", 'correctness': 3}\n",
      "\n",
      " Expecting output: 4 \n",
      "\n",
      "{'reasoning': \"The evaluation documentation provides detailed explanations for each field in the 'MarketplaceDailySnapshot' table schema. This includes comprehensive descriptions of each column which are more detailed than the gold documentation. For example, the 'id' field adds context by specifying the structure: '{ Contract address }-{# of days since Unix epoch time}', while the gold documentation lacks this detail. Additionally, the evaluation documentation includes explanations of the data type formatting such as 'in Ether, formatted as a decimal', which increases clarity. These elaborations ensure that the documentation exceeds the gold standard by offering more comprehensive context and detail. Furthermore, the descriptions adhere to proper grammatical structuring with complete sentences, making them clear and easy to understand.\", 'correctness': 4}\n",
      "\n",
      " Expecting output: 3 \n",
      "\n",
      "{'reasoning': 'To evaluate the quality of the documentation provided in the EVALUATION DOCUMENTATION, we must compare it against the GOLD DOCUMENTATION using the specified correctness criterion. \\n\\n1. **Correctness and Completeness**: Both the EVALUATION DOCUMENTATION and GOLD DOCUMENTATION are provided in the form of type definitions with comments explaining each field. The two documents are identical line by line. Each field in the schema is clearly defined with accurate and descriptive comments that explain its meaning and purpose. This makes the documentation complete and free from ambiguity. As a result, the documentation clearly explains what each field represents. \\n\\n2. **Grammar and Clarity**: Both documents follow proper grammatical rules with correct capitalization and punctuation. The use of full English sentences is consistent throughout the documentation. This enhances readability and clarity, meeting the criterion for Perfect documentation.\\n\\n3. **Comparison with GOLD**: Since the EVALUATION DOCUMENTATION matches the GOLD DOCUMENTATION exactly and all step-by-step checks confirm that the comments are clear, complete, and correctly formatted, it meets the criteria for Perfect documentation.\\n\\nBased on this detailed analysis, the provided EVALUATION DOCUMENTATION not only matches the GOLD DOCUMENTATION but meets all the additional criteria required for Perfect column descriptions. Therefore, its correctness rating should be 4.', 'correctness': 4}\n",
      "\n",
      " Expecting output: 2 \n",
      "\n",
      "{'reasoning': \"To determine the correctness of the EVALUATION DOCUMENTATION as compared to the GOLD DOCUMENTATION, we need to evaluate it based on the criteria provided. \\n\\n1. Correctness:\\n   - The ID field: Both descriptions denote that it is of type ID and indicate it relates to {Contract address}-{# of days since epoch time}, although the GOLD DOCUMENTATION specifically includes 'Unix'. This is a minor detail, thus the EVALUATION DOCUMENTATION is still clear.\\n   - The marketplace field: The GOLD DOCUMENTATION specifies 'The marketplace that this snapshot belongs to,' whereas the EVALUATION simply states 'The marketplace.' The GOLD version offers a bit more context, making it clearer, but the EVALUATION's isn't incorrect.\\n   - Block number and timestamp fields: Both documentations describe these adequately, with only slight differences in wording that don't introduce any ambiguity.\\n   - The collectionCount, cumulativeTradeVolumeETH, tradeCount, cumulativeUniqueTraders, dailyActiveTraders, dailyTradedCollectionCount, and dailyTradedItemCount fields: The descriptions are similar, with the GOLD offering more precise or slightly more informative descriptions. However, these differences are minor and don't create confusion.\\n   - The marketplaceRevenueETH, creatorRevenueETH, and totalRevenueETH fields: The GOLD DOCUMENTATION includes clarifications (e.g., 'aka protocol fee'), which could enhance understanding, but the absence of these doesn't make the EVALUATION version incorrect.\\n\\n2. Ambiguity and Information:\\n   - The EVALUATION DOCUMENTATION generally lacks the additional context provided in the GOLD DOCUMENTATION. The GOLD tends to slightly expand on certain fields, such as clarifying what 'revenue' or 'trade count' pertains to. However, none of this absence in EVALUATION leads to incorrect or misleading information.\\n\\nOverall, while the EVALUATION DOCUMENTATION is less detailed than the GOLD DOCUMENTATION, it doesn't present any incorrect information; rather, it lacks some specificity and contextual depth provided in the GOLD.\\n\\nTherefore, the EVALUATION DOCUMENTATION can be rated as 'Almost Perfect' as it closely matches the GOLD DOCUMENTATION, although not to the level of detail that would be perfect documentation.\", 'correctness': 3}\n",
      "\n",
      " Expecting output: 1 \n",
      "\n",
      "{'reasoning': \"The evaluation documentation and the gold documentation are provided for a table schema, and the goal is to evaluate the correctness of the evaluation documentation against the gold documentation using the provided criteria. Let's assess each component:\\n\\n1. **id**: The evaluation documentation lacks a detailed description, while the gold documentation includes context about the composition of the ID, mentioning it combines 'Contract address' and '# of days since Unix epoch time'. This additional information provides clarity that is missing in the evaluation documentation.\\n\\n2. **marketplace**: The evaluation describes it merely as 'marketplace', whereas the gold documentation provides complete context 'The marketplace that this snapshot belongs to'. The gold documentation offers better clarity about the reference.\\n\\n3. **blockNumber**: The gold documentation provides a specific context that the block number is 'where the snapshot is taken', which helps in understanding the temporal aspect of when the block is recorded. This is absent in the evaluation documentation.\\n\\n4. **timestamp**: Similar to the blockNumber, the gold documentation describes it as 'Block timestamp when the snapshot is taken', specifying the importance of the timestamp in relation to the snapshot event, which the evaluation documentation fails to include.\\n\\n5. **collectionCount**: The evaluation documentation is vague with 'collections on marketplace', while the gold explicitly states 'Number of collections listed on the marketplace', which provides a complete description.\\n\\n6. **cumulativeTradeVolumeETH**: The evaluation documentation simply states 'trade volume', whereas the gold version clarifies it as 'Cumulative trade volume (in ETH)', specifying the measurement unit and cumulative nature.\\n\\n7. **marketplaceRevenueETH**: The evaluation provides 'creator fee', which seems to be incorrect and misleading. The gold documentation gives 'Revenue that goes to the marketplace protocol, aka protocol fee', clarifying the meaning and aligning with the column name.\\n\\n8. **creatorRevenueETH**: Similarly, the evaluation gives 'marketplace fee', which is misleading, while the gold documentation states 'Revenue that goes to creator, aka royalty fee', providing clarity and correcting the misinterpretation.\\n\\n9. **totalRevenueETH**: The gold documentation explains it as the 'Sum of marketplaceRevenueETH and creatorRevenueETH', giving a complete understanding that the evaluation documentation lacks.\\n\\n10. **tradeCount**: The evaluation's 'trade count of traders' is misleading. The gold documentation clarifies it as the 'Trade count of all collections on the marketplace', providing accurate context and target scope.\\n\\n11. **cumulativeUniqueTraders**: The evaluation simply names it without extra context. However, the gold mentions it straightforwardly 'Cumulative unique traders', which are equivalent but the gold standard provides clearer description grammar.\\n\\n12. **dailyActiveTraders**: Both the evaluation and the gold documentation provide similar descriptions. However, minor issues in capitalization exist in the evaluation version.\\n\\n13. **dailyTradedCollectionCount**: The gold specifies 'Number of traded collections of the day', while the evaluation leaves out 'Number' leading to a slightly incomplete understanding.\\n\\n14. **dailyTradedItemCount**: Similar to the point above, the evaluation misses the clarity that gold provides by stating 'Number of traded items of the day'.\\n\\nOverall, the evaluation documentation is lacking in clarity, context, and detail compared to the gold documentation, making it a somewhat correct representation due to missing and sometimes misleading descriptions.\", 'correctness': 2}\n"
     ]
    }
   ],
   "source": [
    "# Function to reload the template\n",
    "def reload_template(template_path=\"../assets/prompts\", template_name=\"entity_comparison_prompt.txt\"):\n",
    "    env = Environment(loader=FileSystemLoader(template_path))\n",
    "    env.cache.clear()\n",
    "    entity_comparison_prompt_template = env.get_template(template_name)\n",
    "    return entity_comparison_prompt_template\n",
    "\n",
    "def parse_response(response): \n",
    "    response = response.choices[0].message.content\n",
    "    response = response.strip('```json').strip('```').strip()\n",
    "    response = json.loads(response)\n",
    "    return response\n",
    "\n",
    "example_prompt = reload_template()\n",
    "example_prompt_filled = example_prompt.render({\"entity_pred\": \"<example prediction goes here>\", \"entity_gold\": \"<example gold goes here>\"})\n",
    "\n",
    "entity_comparison_prompt_template = reload_template()\n",
    "gold_output = entity_comparison_prompt_template.render({\"entity_pred\": {gold_entity_comparison}, \"entity_gold\": {gold_entity_comparison}})\n",
    "four_output = entity_comparison_prompt_template.render({\"entity_pred\": {four_entity_comparison}, \"entity_gold\": {gold_entity_comparison}})\n",
    "three_output = entity_comparison_prompt_template.render({\"entity_pred\": {three_entity_comparison}, \"entity_gold\": {gold_entity_comparison}})\n",
    "two_output = entity_comparison_prompt_template.render({\"entity_pred\": {two_entity_comparison}, \"entity_gold\": {gold_entity_comparison}})\n",
    "one_output = entity_comparison_prompt_template.render({\"entity_pred\": {one_entity_comparison}, \"entity_gold\": {gold_entity_comparison}})\n",
    "\n",
    "print(\"Expecting output: 3 \\n\")\n",
    "gold_comparison = parse_response(gpt_chat_completion(client, gold_output))\n",
    "print(gold_comparison)\n",
    "\n",
    "print(\"\\n Expecting output: 4 \\n\")\n",
    "four_comparison = parse_response(gpt_chat_completion(client, four_output))\n",
    "print(four_comparison)\n",
    "\n",
    "print(\"\\n Expecting output: 3 \\n\")\n",
    "three_comparison = parse_response(gpt_chat_completion(client, three_output))\n",
    "print(three_comparison)\n",
    "\n",
    "print(\"\\n Expecting output: 2 \\n\")\n",
    "two_comparison = parse_response(gpt_chat_completion(client, two_output))\n",
    "print(two_comparison)\n",
    "\n",
    "print(\"\\n Expecting output: 1 \\n\")\n",
    "one_comparison = parse_response(gpt_chat_completion(client, one_output))\n",
    "print(one_comparison)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_comparison_prompt_template = reload_template(\n",
    "    template_path=\"../assets/prompts\",\n",
    "    template_name=\"entity_comparison_revision.txt\",\n",
    ")\n",
    "\n",
    "entity_comparison_prompt_modification = entity_comparison_prompt_template.render(\n",
    "    {\n",
    "        \"original_prompt\": {example_prompt_filled}, \n",
    "\n",
    "        \"four_result_correct\": {\"True\" if four_comparison[\"correctness\"] == 4 else \"False\"},\n",
    "        \"four_result\": {four_comparison[\"reasoning\"]},\n",
    "        \n",
    "        \"three_result_correct\": {\"True\" if three_comparison[\"correctness\"] == 3 else \"False\"},\n",
    "        \"three_result\": {three_comparison[\"reasoning\"]},\n",
    "        \n",
    "        \"two_result_correct\": {\"True\" if two_comparison[\"correctness\"] == 2 else \"False\"},\n",
    "        \"two_result\": {two_comparison[\"reasoning\"]},\n",
    "        \n",
    "        \"one_result_correct\": {\"True\" if one_comparison[\"correctness\"] == 1 else \"False\"},\n",
    "        \"one_result\": {one_comparison[\"reasoning\"]},\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[TASK]: Your job is to revise the prompt to better meet our goal of determining the quality of documentation. Based upon the results and reasoning provided by the model, update the prompt we are using to score the documentation. \\n\\nIt is important that the prompt addresses that the comparison should label the EVALUATION DOCUMENTATION in comparison to the GOLD DOCUMENTATION by a scale of 1-4. \\n***\\n[ORIGINAL PROMPT]: {\\'You are evaluating the quality of documentation between two representations of the same schema. One schema will be provided as an example of quality documentation. The other table schema is the one you will be evaluating. \\\\n[BEGIN DATA]\\\\n***\\\\n[TASK]: The task is to determine the quality of documentation for one table schema, given access only to a table definition and two different documentation implementations. \\\\n\\\\nThe goal is to create informative descriptions which reduces ambiguity and increases understanding for users of the database .\\\\n\\\\n***\\\\n[EVALUATION DOCUMENTATION]: <example prediction goes here>\\\\n***\\\\n[GOLD DOCUMENTATION]: <example gold goes here>\\\\n***\\\\n[DOCUMENTATION CRITERION]: Evaluation Criteria\\\\n\\\\nCorrectness: \\\\n4: Perfect ( Exceeding the GOLD documentation ):\\\\nA perfect column description, and in turn table documentation, should contain enough information so that the interpretation of the column is completely free of ambiguity. It does not need to include any descriptions of the specific values inside the column to be considered perfect. The description should contain information about what table the column is referencing. For example, instead of \"The name,\" we want \"The name of the client that made the transaction\" if we have a transaction database with columns such as NAME, AMOUNT, and DATE to resolve the ambiguity of what the name refers to. Additionally, the column description should be a full and valid English sentence, with proper grammar, capitalization, and punctuation. For instance, instead of \"nationality of drivers\" when each instance refers to only one driver, it should be \"The nationality of a driver.\"\\\\n\\\\n3: Almost Perfect ( Matching the GOLD documentation ):\\\\nMatching the gold description, but failing to meet the criteria set by perfect column descriptions. The column descriptions are somewhat correct, but there is room for improvement.\\\\n\\\\n2: Somewhat Correct ( Falling below the GOLD documentation )\\\\nColumn descriptions are somewhat correct, but there is room for improvement due to missing or misleading information. It could still contain correct information, but any incorrect information automatically leads to a somewhat correct rating.\\\\n\\\\n1: Incorrect  ( Invalid based upon the GOLD documentation )\\\\nThe column descriptions are missing or completely misleading. The documentation not only fails to accurately describe the table and it\\\\\\'s columns, it is completely incorrect. An incorrect column clearly has a description that does not align with the column naming.\\\\n\\\\n***\\\\n[ END DATA ]\\\\n\\\\nDoes the submission meet the criterion? First, write out in a step by step manner your reasoning about the criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset.\\\\n\\\\nYour response must be a JSON object following this schema:  \\\\n\\\\n{ \"reasoning\": str, \"correctness\": int }  \\\\n\\\\nMake sure your output is a valid JSON object in the format provided above.\\'}\\n***\\n\\n[Expected Output of 4 Was Correct]\\n\\n***\\n\\n[Expected Output of 3 Was Correct]\\n\\n***\\n\\n[Expected Output of 2 Was Correct]\\n\\n***\\n\\n[Expected Output of 1 Was Correct]\\n\\n***\\n[OBJECTIVE]: Based upon the results above, provide an updated prompt that will better align with the outputs we expect and those we are getting. \\n***\\n[RESULT]: *respond with only the modified prompt*'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entity_comparison_prompt_modification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_revision = gpt_chat_completion(client, entity_comparison_prompt_modification).choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[TASK]: Your job is to revise the prompt to better meet our goal of determining the quality of documentation. Based upon the results and reasoning provided by the model, update the prompt we are using to score the documentation. \\n\\nIt is important that the prompt addresses that the comparison should label the EVALUATION DOCUMENTATION in comparison to the GOLD DOCUMENTATION by a scale of 1-4. \\n***\\n[ORIGINAL PROMPT]: {<Template \\'entity_comparison_revision.txt\\'>}\\n***\\n[Expected Output of 4]: {\\'```json\\\\n{\\\\n    \"reasoning\": \"The evaluation documentation provides detailed descriptions for each field in the MarketplaceDailySnapshot schema. Let\\\\\\'s compare each field with the gold documentation to determine how it meets the criterion.\\\\\\\\n\\\\\\\\n1. \\\\\\'id\\\\\\' field: The evaluation documentation describes the structure of the ID in detail, which exceeds the information provided in the gold documentation. This improves clarity and reduces ambiguity.\\\\\\\\n\\\\\\\\n2. \\\\\\'marketplace\\\\\\' field: The evaluation documentation offers a slightly more detailed explanation, specifying it as a \\\\\\'reference to the marketplace entity.\\\\\\' This adds context not present in the gold documentation.\\\\\\\\n\\\\\\\\n3. \\\\\\'blockNumber\\\\\\' field: The evaluation provides a more detailed description, stating it refers to the \\\\\\'block that the transaction that the snapshot was taken in,\\\\\\' which enhances clarity.\\\\\\\\n\\\\\\\\n4. \\\\\\'timestamp\\\\\\' field: Similar to blockNumber, this description is more detailed, specifying it is \\\\\\'associated with the block that the snapshot transaction was taken in,\\\\\\' which provides more context.\\\\\\\\n\\\\\\\\n5. \\\\\\'collectionCount\\\\\\' field: Both documentations are quite similar, but the evaluation mentions \\\\\\'OpenSea marketplace,\\\\\\' which adds specificity.\\\\\\\\n\\\\\\\\n6. \\\\\\'cumulativeTradeVolumeETH\\\\\\' field: The evaluation description adds information by noting it is formatted as a decimal, which is additional useful detail.\\\\\\\\n\\\\\\\\n7. \\\\\\'marketplaceRevenueETH\\\\\\' field: This description is better because it clarifies that it is \\\\\\'aka protocol fee,\\\\\\' aligning closely and adding context to the gold documentation.\\\\\\\\n\\\\\\\\n8. \\\\\\'creatorRevenueETH\\\\\\' field: Similar to marketplaceRevenueETH, the evaluation adds specificity and clarity by stating the fee\\\\\\'s nature.\\\\\\\\n\\\\\\\\n9. \\\\\\'totalRevenueETH\\\\\\' field: The evaluation explicitly states the components of the total revenue, \\\\\\'Marketplace revenue and the Creator revenue,\\\\\\' which gives a clearer understanding.\\\\\\\\n\\\\\\\\n10. \\\\\\'tradeCount\\\\\\' field: \\\\\\'Of all collections on the marketplace protocol\\\\\\' is more precise and detailed than the gold documentation.\\\\\\\\n\\\\\\\\n11. \\\\\\'cumulativeUniqueTraders\\\\\\' field: The evaluation adds a precise definition as \\\\\\'a count of unique addresses that have traded,\\\\\\' improving understanding.\\\\\\\\n\\\\\\\\n12. \\\\\\'dailyActiveTraders\\\\\\' field: The description \\\\\\'based upon a count of user addresses\\\\\\' provides additional detail to the meaning of active traders.\\\\\\\\n\\\\\\\\n13. \\\\\\'dailyTradedCollectionCount\\\\\\' and \\\\\\'dailyTradedItemCount\\\\\\' fields: The evaluation adds the specific temporal context \\\\\\'at the time of the snapshot,\\\\\\' making the statement clearer.\\\\\\\\n\\\\\\\\nIn general, the evaluation documentation exceeds the gold documentation by providing additional clarity and context on each field. The descriptions are complete, unambiguous, and conform to proper grammar and sentence structure.\",\\\\n    \"correctness\": 4\\\\n}\\\\n```\\'}\\n***\\n[Expected Output of 3]: {\\'```json\\\\n{\\\\n    \"reasoning\": \"To evaluate the documentation quality, we will compare each column description from the EVALUATION DOCUMENTATION to the GOLD DOCUMENTATION. We start by noting that all column descriptions in the EVALUATION DOCUMENTATION are identical to those in the GOLD DOCUMENTATION. Each column description provides clear and specific information about what it represents, reducing ambiguity and enhancing understanding for the user. For example, the \\\\\\'id\\\\\\' column is described using a format combining \\\\\\'Contract address\\\\\\' and \\\\\\'# of days since Unix epoch time\\\\\\', which is both precise and informative. Similarly, other columns such as \\\\\\'marketplace\\\\\\', \\\\\\'blockNumber\\\\\\', \\\\\\'timestamp\\\\\\', \\\\\\'collectionCount\\\\\\', and so on, have descriptions that are complete sentences, with correct grammar, capitalization, and punctuation, matching the GOLD documentation. Since both the EVALUATION DOCUMENTATION and the GOLD DOCUMENTATION are identical, and both meet the criteria for \\\\\\'Almost Perfect\\\\\\', we conclude that the EVALUATION DOCUMENTATION is indeed at the same level as the GOLD documentation and falls under the \\\\\\'Almost Perfect\\\\\\' category.\",\\\\n    \"correctness\": 3\\\\n}\\\\n```\\'}\\n***\\n[Expected Output of 2]: {\\'```json\\\\n{\\\\n  \"reasoning\": \"To evaluate the quality of the documentation, we will compare the EVALUATION DOCUMENTATION against the GOLD DOCUMENTATION based on the criteria provided. \\\\\\\\n\\\\\\\\n1. The description for the \\\\\\'id\\\\\\' field in both the Evaluation and Gold Documentation is very similar, but the Gold Documentation uses a more precise phrasing with \\\\\\'Unix epoch time.\\\\\\' This aligns with perfect column descriptions where context is explicitly provided.\\\\\\\\n\\\\\\\\n2. The \\\\\\'marketplace\\\\\\' field is described as \\\\\\'The marketplace.\\\\\\' in the Evaluation Documentation, whereas the Gold Documentation provides a clearer context: \\\\\\'The marketplace that this snapshot belongs to.\\\\\\' This difference impacts the clarity and makes the Evaluation Documentation less informative.\\\\\\\\n\\\\\\\\n3. For \\\\\\'blockNumber\\\\\\' and \\\\\\'timestamp,\\\\\\' both documentations are similar. However, the Gold version provides a slightly more explicit description with \\\\\\'where the snapshot is taken\\\\\\' and \\\\\\'when the snapshot is taken\\\\\\', adding clarity.\\\\\\\\n\\\\\\\\n4. The field \\\\\\'collectionCount\\\\\\' in the Evaluation Documentation is \\\\\\'collections count on marketplace.\\\\\\' In contrast, the Gold Documentation provides a more grammatically correct and detailed description as \\\\\\'Number of collections listed on the marketplace.\\\\\\'\\\\\\\\n\\\\\\\\n5. Descriptions for \\\\\\'cumulativeTradeVolumeETH\\\\\\', \\\\\\'marketplaceRevenueETH\\\\\\', \\\\\\'creatorRevenueETH\\\\\\', and \\\\\\'totalRevenueETH\\\\\\' in the Evaluation Documentation are less detailed compared to the Gold Documentation. The Gold version provides explanations where terms like \\\\\\'protocol fee\\\\\\' and \\\\\\'royalty fee\\\\\\' are elaborated upon, enhancing the understanding of each column.\\\\\\\\n\\\\\\\\n6. The \\\\\\'tradeCount\\\\\\' description in the Evaluation Documentation lacks specificity compared to the Gold: \\\\\\'Trade count of the all collections on the marketplace,\\\\\\' which provides a complete thought consistent with perfect documentation criteria.\\\\\\\\n\\\\\\\\n7. For \\\\\\'cumulativeUniqueTraders\\\\\\', \\\\\\'dailyActiveTraders\\\\\\', \\\\\\'dailyTradedCollectionCount\\\\\\', and \\\\\\'dailyTradedItemCount,\\\\\\' the descriptions are quite similar in both, with slight variations in wording, but the Gold Documentation often uses complete sentences, improving grammatical correctness.\\\\\\\\n\\\\\\\\nIn conclusion, while the Evaluation Documentation is generally correct, it usually lacks the detailed context and full sentences employed in the Gold Documentation. Thus, the Evaluation Documentation does not meet the criteria to match the GOLD documentation.\",\\\\n  \"correctness\": 2\\\\n}\\\\n```\\'}\\n***\\n[Expected Output of 1]: {\\'```json\\\\n{\\\\n  \"reasoning\": \"To evaluate the quality of the documentation provided in the EVALUATION DOCUMENTATION, we need to compare it to the GOLD DOCUMENTATION according to the given criteria. The goal is to determine if the descriptions are clear, complete, and grammatically correct, and if they reduce ambiguity. \\\\\\\\n\\\\\\\\n1. The \\\\\\'id\\\\\\' field in the EVALUATION DOCUMENTATION lacks a description. This is a significant omission compared to the GOLD DOCUMENTATION, which provides a detailed explanation of the ID format.\\\\\\\\n\\\\\\\\n2. The \\\\\\'marketplace\\\\\\' field in the EVALUATION DOCUMENTATION has a very basic description (\\\\\\'marketplace: Marketplace!\\\\\\') with no added context, whereas the GOLD DOCUMENTATION explains that it is the marketplace this snapshot belongs to.\\\\\\\\n\\\\\\\\n3. For most other fields like \\\\\\'collectionCount\\\\\\', \\\\\\'cumulativeTradeVolumeETH\\\\\\', \\\\\\'marketplaceRevenueETH\\\\\\', etc., the EVALUATION DOCUMENTATION provides descriptions but they are brief and not as informative as in the GOLD DOCUMENTATION. For example, \\\\\\'trade volume\\\\\\' versus \\\\\\'Cumulative trade volume (in ETH)\\\\\\'. These descriptions could lead to ambiguity because they do not provide enough context or specificity.\\\\\\\\n\\\\\\\\n4. The EVALUATION DOCUMENTATION sometimes uses informal or unclear language. For example, \\\\\\'creator fee\\\\\\' is less informative compared to \\\\\\'Revenue that goes to creator, aka royalty fee.\\\\\\'\\\\\\\\n\\\\\\\\nOverall, while the EVALUATION DOCUMENTATION has brief descriptions, they often lack the detail and precision required to be as informative as the GOLD DOCUMENTATION. Several key descriptions, such as the \\\\\\'id\\\\\\' and \\\\\\'marketplace\\\\\\', are completely missing, leading to potential ambiguity.\\\\\\\\n\\\\\\\\nGiven these observations, the EVALUATION DOCUMENTATION falls below the standard of the GOLD DOCUMENTATION due to missing and insufficiently detailed descriptions. Thus, it meets the criteria for \\\\\\'Somewhat Correct\\\\\\'.\",\\\\n  \"correctness\": 2\\\\n}\\\\n```\\'}\\n***\\n[OBJECTIVE]: Based upon the results above, provide an updated prompt that will better align with the outputs we expect and those we are getting. \\n***\\n[RESULT]: *respond with only the modified prompt*'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entity_comparison_prompt_modification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"You are tasked with evaluating the quality of documentation between two representations of the same schema. One schema is provided as the benchmark (GOLD documentation), and you will evaluate the other based on this benchmark.\\\\n\\\\n[BEGIN DATA]\\\\n***\\\\n[TASK]: Your task is to determine the quality of the EVALUATION DOCUMENTATION for a table schema, using only a table definition and two different documentation implementations. The goal is to create informative descriptions that reduce ambiguity and increase understanding for users of the database.\\\\n\\\\n[EVALUATION DOCUMENTATION]: <example prediction goes here>\\\\n***\\\\n[GOLD DOCUMENTATION]: <example gold goes here>\\\\n***\\\\n[DOCUMENTATION CRITERION]: Evaluation Criteria\\\\n\\\\nCorrectness: \\\\n4: Perfect (Exceeding the GOLD documentation):\\\\nThe description provides full clarity without ambiguity, describing what each column references. It uses complete English sentences with proper grammar, capitalization, and punctuation.\\\\n\\\\n3: Almost Perfect (Matching the GOLD documentation):\\\\nThe description matches the GOLD documentation but lacks some elements that would elevate it to perfection. Improvements can be made.\\\\n\\\\n2: Somewhat Correct (Falling below the GOLD documentation):\\\\nThe description is partially correct but contains missing or misleading information, leading to a lower rating.\\\\n\\\\n1: Incorrect (Invalid based upon the GOLD documentation):\\\\nThe description is misleading or fails entirely in accuracy. It does not align with column naming or intent.\\\\n\\\\n***\\\\n[END DATA]\\\\n\\\\nUse the criteria to assess the EVALUATION DOCUMENTATION in comparison to the GOLD DOCUMENTATION, and provide a score from 1 to 4.\\\\n\\\\nYour response must be a JSON object following this schema: \\\\n\\\\n{ \\\\\"reasoning\\\\\": str, \\\\\"correctness\\\\\": int }\\\\n\\\\nEnsure your output is a valid JSON object in the format provided above.\"}'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_revision"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "graphdoc-x8ppHhEw-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
