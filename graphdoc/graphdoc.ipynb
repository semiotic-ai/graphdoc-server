{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello, world!\n"
     ]
    }
   ],
   "source": [
    "print(\"hello, world!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# system packages \n",
    "import os\n",
    "import json\n",
    "\n",
    "# internal packages \n",
    "\n",
    "# external packages\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "from graphql import build_schema, parse\n",
    "from jinja2 import Environment, FileSystemLoader\n",
    "\n",
    "load_dotenv()\n",
    "open_ai_api_key = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GraphQL Schema Parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# with open('../assets/schemas/opensea/opensea_original_schema.graphql', 'r') as schema_file:\n",
    "#     schema_str = schema_file.read()\n",
    "\n",
    "# # TEMP: Try to parse the schema given the errors we are seeing\n",
    "# custom_definitions = '''\n",
    "# scalar BigDecimal\n",
    "# scalar BigInt\n",
    "\n",
    "# directive @entity on OBJECT\n",
    "# directive @dailySnapshot on OBJECT\n",
    "# directive @regularPolling on OBJECT\n",
    "# directive @derivedFrom on OBJECT\n",
    "# directive @transaction on OBJECT\n",
    "# '''\n",
    "\n",
    "# full_schema_str = custom_definitions + schema_str\n",
    "# schema = build_schema(full_schema_str)\n",
    "\n",
    "# for type_name, graphql_type in schema.type_map.items():\n",
    "#     if type_name.startswith('__'):\n",
    "#         continue  # Skip introspection types\n",
    "#     print(f\"Type: {type_name}\")\n",
    "#     for field_name, field in graphql_type.fields.items():\n",
    "#         print(f\"  Field: {field_name} (type: {field.type})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LangGraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import getpass\n",
    "# import os\n",
    "\n",
    "# if \"OPENAI_API_KEY\" not in os.environ:\n",
    "#     os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter your OpenAI API key: \")\n",
    "# open_ai_api_key = os.environ[\"OPENAI_API_KEY\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from typing import Annotated\n",
    "\n",
    "# from langchain_openai import OpenAI\n",
    "# from typing_extensions import TypedDict\n",
    "\n",
    "# from langgraph.graph import StateGraph\n",
    "# from langgraph.graph.message import add_messages\n",
    "\n",
    "\n",
    "# class State(TypedDict):\n",
    "#     messages: Annotated[list, add_messages]\n",
    "\n",
    "\n",
    "# graph_builder = StateGraph(State)\n",
    "\n",
    "\n",
    "# llm = OpenAI(model=\"gpt-4\")\n",
    "\n",
    "\n",
    "# def chatbot(state: State):\n",
    "#     return {\"messages\": [llm.invoke(state[\"messages\"])]}\n",
    "\n",
    "# graph_builder.add_node(\"chatbot\", chatbot)\n",
    "# graph_builder.set_entry_point(\"chatbot\")\n",
    "# graph_builder.set_finish_point(\"chatbot\")\n",
    "# graph = graph_builder.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from IPython.display import Image, display\n",
    "\n",
    "# try:\n",
    "#     display(Image(graph.get_graph().draw_mermaid_png()))\n",
    "# except Exception:\n",
    "#     pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OpenAI Directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! I'm just a program, so I don't have feelings, but I'm here and ready to help you. How can I assist you today?\n"
     ]
    }
   ],
   "source": [
    "client = OpenAI(\n",
    "    api_key=os.environ.get(\"OPENAI_API_KEY\"),  # This is the default and can be omitted\n",
    ")\n",
    "\n",
    "def gpt_chat_completion(client, message, model=\"gpt-4o\"):\n",
    "    return client.chat.completions.create(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": message,\n",
    "        }\n",
    "    ],\n",
    "    model=model,\n",
    ")\n",
    "\n",
    "print(gpt_chat_completion(client, \"Hello! How are you?\").choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial Test Set \n",
    "\n",
    "### Prompts \n",
    "\n",
    "- [ ] One entity quality vs. another entity quality \n",
    "- [ ] One column quality vs. another column quality \n",
    "- [ ] One schema quality vs. another schema quality \n",
    "\n",
    "### Demonstrations \n",
    "\n",
    "- [ ] A high quality entity (description)\n",
    "- [ ] A high quality column (description)\n",
    "- [ ] A high quality schema (description)\n",
    "\n",
    "### Quality Categories\n",
    "\n",
    "- [ ] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"tests/assets/entity_comparison_assets.json\", \"r\") as f:\n",
    "    entity_comparison_assets = json.load(f)\n",
    "\n",
    "gold_entity_comparison = \"\".join(entity_comparison_assets[\"gold\"][\"prompt\"])\n",
    "four_entity_comparison = \"\".join(entity_comparison_assets[\"four\"][\"prompt\"])\n",
    "three_entity_comparison = \"\".join(entity_comparison_assets[\"three\"][\"prompt\"])\n",
    "two_entity_comparison = \"\".join(entity_comparison_assets[\"two\"][\"prompt\"])\n",
    "one_entity_comparison = \"\".join(entity_comparison_assets[\"one\"][\"prompt\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expecting output: 3 \n",
      "\n",
      "```json\n",
      "{\n",
      "  \"reasoning\": \"To evaluate the quality of the given documentation against the gold standard, we will compare each element step by step:\\n\\n1. **Structure and Format**: Both the evaluation documentation and the gold documentation share a similar structure where each attribute of the 'MarketplaceDailySnapshot' is followed by a detailed description.\\n\\n2. **Content**: The evaluation documentation's content for each field matches perfectly with the gold documentation. Both provide precise and unambiguous explanations for each column.\\n\\n   - **ID**: The description '{ Contract address }-{# of days since Unix epoch time}' provides necessary context and is precisely the same in both cases.\\n   - **Marketplace**: Both describe it as 'The marketplace that this snapshot belongs to.', adequately explaining the field's purpose.\\n   - **BlockNumber and Timestamp**: The explanations of when the snapshot is taken, at what block number and timestamp, are clearly detailed and identical.\\n   - **CollectionCount**: The description 'Number of collections listed on the marketplace.' is present and correctly conveys the meaning.\\n   - **CumulativeTradeVolumeETH, MarketplaceRevenueETH, CreatorRevenueETH, TotalRevenueETH**: Each description regarding different types of revenues and volumes offers a comprehensive understanding of the data they represent and matches word for word.\\n   - **TradeCount and CumulativeUniqueTraders, DailyActiveTraders**: These are clearly described in both versions without room for ambiguity.\\n   - **DailyTradedCollectionCount and DailyTradedItemCount**: The explanations provided make it clear that these refer to daily metrics of collections and items, and they are identical in both documents.\\n\\n3. **Correctness**: Based on the evaluation criteria, the documentation perfectly matches the gold standard, which exemplifies accurate documentation. There is no missing, misleading or incorrect information, and the syntax is grammatically sound and structured.\\n\\nGiven this detailed step-by-step comparison, the evaluation documentation surpasses the basic requirement, achieving perfect documentation as defined by the criterion.\",\n",
      "  \"correctness\": 4\n",
      "}\n",
      "```\n",
      "\n",
      " Expecting output: 4 \n",
      "\n",
      "```json\n",
      "{\n",
      "    \"reasoning\": \"Let's evaluate the documentation based on the provided criteria.\\n\\n1. **Correctness**:\\n   - The evaluated documentation generally provides detailed descriptions. However, we need to assess how close it is to being 'Perfect' as defined in the criteria or whether it matches the 'Gold' documentation.\\n\\n2. **Detailed Comparison**:\\n   - **ID**: The evaluated documentation provides extra context about the structure of the ID, specifying the format as '{ Contract address }-{# of days since Unix epoch time}'. This exceeds the GOLD documentation, which simply mentions the format without additional context.\\n   - **Marketplace**: Both descriptions effectively convey that this refers to the associated marketplace. The evaluated version adds 'entity' for clarity.\\n   - **Block Number and Timestamp**: The evaluated documentation provides detailed information about the network and transaction context, which is a step above the Gold description's simpler version.\\n   - **Collection Count, Trade Count, Daily Traded Collection Count, and Daily Traded Item Count**: Both versions essentially match each other, with slightly more detailed wording in the evaluated documentation to ensure clarity on the marketplace and protocol references.\\n   - **Cumulative Trade Volume, Marketplace Revenue, Creator Revenue, and Total Revenue**: The evaluated documentation expands upon terms like 'protocol operations' and ensures clarity by indicating Ether is represented as a decimal. This adds a layer of understanding absent in the Gold version.\\n   - **Cumulative Unique Traders and Daily Active Traders**: The evaluated version describes these in a more granular manner by mentioning user addresses and the protocol aspect, not covered by the Gold documentation.\\n\\n3. **Grammar and Style**:\\n   - The evaluated documentation uses complete sentences, has proper punctuation, and uses capitalization according to standard English rules.\\n\\n4. **Conclusion**:\\n   - Overall, the evaluated documentation provides a higher level of detail and context compared to the Gold standard. It resolves ambiguities that exist in the Gold documentation by giving more specific information about entities, formats, and workings related to the database schema.\\n\\nThus, the evaluated documentation merits a 'Perfect' rating according to the criteria.\",\n",
      "    \"correctness\": 4\n",
      "}\n",
      "```\n",
      "\n",
      " Expecting output: 3 \n",
      "\n",
      "```json\n",
      "{\n",
      "    \"reasoning\": \"To evaluate the quality of the documentation, we need to compare the 'EVALUATION DOCUMENTATION' against the 'GOLD DOCUMENTATION'. Both documents should describe the same schema, which involves various aspects of a 'MarketplaceDailySnapshot'. Each field in the schema is described in terms of its purpose and meaning. \\n\\n1. Start with the 'id' field: Both documents provide the same description, which refers to a unique identifier based on a combination of the contract address and days since Unix epoch time.\\n\\n2. The 'marketplace' field: Both documents describe that it identifies the marketplace associated with the snapshot.\\n\\n3. The 'blockNumber' and 'timestamp' fields: Both documents describe these fields accurately, mentioning that they are related to the block number and timestamp when the snapshot is taken.\\n\\n4. The 'collectionCount' field: Both documents mention this field accurately, describing the number of collections listed on the marketplace.\\n\\n5. The 'cumulativeTradeVolumeETH', 'marketplaceRevenueETH', and 'creatorRevenueETH' fields: Both documents describe these fields by explaining that they represent cumulative trade volume in ETH and revenues for the marketplace and creator, respectively. \\n\\n6. The 'totalRevenueETH' field: Both documents convey the same information accurately, by explaining that this field captures the sum of marketplace and creator revenues.\\n\\n7. The 'tradeCount', 'cumulativeUniqueTraders', 'dailyActiveTraders', 'dailyTradedCollectionCount', and 'dailyTradedItemCount' fields: The descriptions provided are identical and accurate in both documents, giving enough information regarding their purpose without ambiguity.\\n\\nOverall, the 'EVALUATION DOCUMENTATION' matches verbatim with the 'GOLD DOCUMENTATION'. There are no differences in the text, and the descriptions are free of ambiguity and complete with proper English sentences, capitalization, and punctuation. Therefore, the evaluation documentation is rated as 'Almost Perfect', as it matches the gold documentation.\",\n",
      "    \"correctness\": 3\n",
      "}\n",
      "```\n",
      "\n",
      " Expecting output: 2 \n",
      "\n",
      "```json\n",
      "{\n",
      "  \"reasoning\": \"To evaluate the quality of the documentation in the EVALUATION DOCUMENTATION against the criteria, we need to analyze each column's description and compare it to the GOLD DOCUMENTATION. \\n\\n1. **id**: Both descriptions are similar, although the GOLD provides a clearer explanation by specifying '# of days since Unix epoch time'. The EVALUATION is slightly less descriptive but still correct. \\n\\n2. **marketplace**: Both descriptions convey the same information. The GOLD adds 'that this snapshot belongs to', which is more informative. \\n\\n3. **blockNumber**: The EVALUATION 'Block number of snapshot' is less precise compared to the GOLD's 'Block number where the snapshot is taken.' The GOLD is clearer about the context. \\n\\n4. **timestamp**: The EVALUATION's 'Block timestamp of snapshot' is less specific than the GOLD's 'Block timestamp when the snapshot is taken'. \\n\\n5. **collectionCount**: The EVALUATION's 'collections count on marketplace' is less precise than the GOLD's 'Number of collections listed on the marketplace.' The GOLD clarifies 'listed'. \\n\\n6. **cumulativeTradeVolumeETH**: Both descriptions mention 'trade volume (in ETH)', but the GOLD adds 'Cumulative', specifying the aggregation method, which is clearer. \\n\\n7. **marketplaceRevenueETH**: The EVALUATION 'protocol fee' is less descriptive than the GOLD's 'Revenue that goes to the marketplace protocol, aka protocol fee.' The GOLD provides clarity about the destination of revenue. \\n\\n8. **creatorRevenueETH**: Similar evaluation with the EVALUATION 'royalty fee' being less explanatory than the GOLD's 'Revenue that goes to creator, aka royalty fee.' \\n\\n9. **totalRevenueETH**: EVALUATION's 'marketplaceRevenueETH and creatorRevenueETH' is vague compared to GOLD's 'Sum of marketplaceRevenueETH and creatorRevenueETH.' The GOLD specifies it as a sum. \\n\\n10. **tradeCount**: The GOLD description adds 'of all collections on the marketplace' providing clarity that is missing in the EVALUATION. \\n\\n11. **cumulativeUniqueTraders**: Both descriptions are identical, but the context in surrounding fields suggests the need for more specificity in a perfect description. \\n\\n12. **dailyActiveTraders**: Both are similar, but again the surrounding context suggests missing specificity that the GOLD does not fully address either. \\n\\n13. **dailyTradedCollectionCount**: The GOLD adds specificity with 'Number of traded collections of the day' compared to the less formal language of the EVALUATION. \\n\\n14. **dailyTradedItemCount**: Similar observation to the previous field regarding better clarity in the GOLD version.\\n\\nThe EVALUATION DOCUMENTATION matches the GOLD DOCUMENTATION in many instances, with slight ambiguities and lack of specificity in comparison. Therefore, a '3' is appropriate according to the description categories.\",\n",
      "  \"correctness\": 3\n",
      "}\n",
      "```\n",
      "\n",
      " Expecting output: 1 \n",
      "\n",
      "```json\n",
      "{\n",
      "  \"reasoning\": \"To evaluate the quality of the documentation, we must compare the provided EVALUATION DOCUMENTATION against the GOLD DOCUMENTATION based on the given criteria.\\n\\n1. **Column by Column Comparison:**\\n   - **id:** The GOLD documentation describes the ID as a combination of a contract address and number of days since Unix epoch time, while the EVALUATION DOCUMENTATION does not provide this level of detail.\\n   - **marketplace:** The EVALUATION DOCUMENTATION lacks the specificity that it is the marketplace to which the snapshot belongs.\\n   - **blockNumber & timestamp:** The GOLD documentation specifies that these are related to where and when the snapshot was taken, respectively, which is missing in the EVALUATION DOCUMENTATION.\\n   - **collectionCount, cumulativeTradeVolumeETH, marketplaceRevenueETH, creatorRevenueETH, totalRevenueETH:** In these instances, the descriptions in EVALUATION DOCUMENTATION are less detailed and lack clarification about ETH, including definitions of protocol and royalty fees.\\n   - **tradeCount:** The EVALUATION DOCUMENTATION states 'trade count of traders' which is misleading because it should refer to the trade count of collections, similar to the GOLD documentation which is clearer.\\n   - **cumulativeUniqueTraders & dailyActiveTraders:** The EVALUATION DOCUMENTATION includes vague descriptions such as 'active traders' rather than specifying that it relates to cumulative unique traders and daily active traders.\\n   - **dailyTradedCollectionCount & dailyTradedItemCount:** The GOLD documentation provides clarity that these counts are for the specific day, which is implied in the EVALUATION DOCUMENTATION but not explicitly stated.\\n\\n2. **Grammar and Completeness:**\\n   - Several descriptions in the EVALUATION DOCUMENTATION are lacking full sentences and proper punctuation, such as missing capitalization and punctuation, which are properly addressed in the GOLD documentation.\\n\\n3. **Conclusion:**\\n   The EVALUATION DOCUMENTATION is lacking in detail and precision compared to the GOLD documentation. This lack of specificity and incomplete sentences mean the documentation falls below the GOLD documentation.\\n\\nBased on these observations, the overall correctness of the EVALUATION DOCUMENTATION is assessed as Somewhat Correct (falling below the GOLD documentation).\",\n",
      "  \"correctness\": 2\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "# Function to reload the template\n",
    "def reload_template(template_path):\n",
    "    env = Environment(loader=FileSystemLoader(template_path))\n",
    "    env.cache.clear()\n",
    "    entity_comparison_prompt_template = env.get_template(\"entity_comparison_prompt.txt\")\n",
    "    return entity_comparison_prompt_template\n",
    "\n",
    "template_path = \"../assets/prompts\"\n",
    "entity_comparison_prompt_template = reload_template(template_path)\n",
    "\n",
    "# env = Environment(loader=FileSystemLoader(\"../assets/prompts\"))\n",
    "# entity_comparison_prompt_template = env.get_template(\"entity_comparison_prompt.txt\")\n",
    "gold_output = entity_comparison_prompt_template.render({\"entity_pred\": {gold_entity_comparison}, \"entity_gold\": {gold_entity_comparison}})\n",
    "four_output = entity_comparison_prompt_template.render({\"entity_pred\": {four_entity_comparison}, \"entity_gold\": {gold_entity_comparison}})\n",
    "three_output = entity_comparison_prompt_template.render({\"entity_pred\": {three_entity_comparison}, \"entity_gold\": {gold_entity_comparison}})\n",
    "two_output = entity_comparison_prompt_template.render({\"entity_pred\": {two_entity_comparison}, \"entity_gold\": {gold_entity_comparison}})\n",
    "one_output = entity_comparison_prompt_template.render({\"entity_pred\": {one_entity_comparison}, \"entity_gold\": {gold_entity_comparison}})\n",
    "\n",
    "print(\"Expecting output: 3 \\n\")\n",
    "print(gpt_chat_completion(client, gold_output).choices[0].message.content)\n",
    "\n",
    "print(\"\\n Expecting output: 4 \\n\")\n",
    "print(gpt_chat_completion(client, four_output).choices[0].message.content)\n",
    "\n",
    "print(\"\\n Expecting output: 3 \\n\")\n",
    "print(gpt_chat_completion(client, three_output).choices[0].message.content)\n",
    "\n",
    "print(\"\\n Expecting output: 2 \\n\")\n",
    "print(gpt_chat_completion(client, two_output).choices[0].message.content)\n",
    "\n",
    "print(\"\\n Expecting output: 1 \\n\")\n",
    "print(gpt_chat_completion(client, one_output).choices[0].message.content)  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "graphdoc-x8ppHhEw-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
