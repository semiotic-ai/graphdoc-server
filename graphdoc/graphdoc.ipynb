{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello, world!\n"
     ]
    }
   ],
   "source": [
    "print(\"hello, world!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# system packages \n",
    "import os\n",
    "import json\n",
    "\n",
    "# internal packages \n",
    "\n",
    "# external packages\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "from graphql import build_schema, parse\n",
    "from jinja2 import Environment, FileSystemLoader\n",
    "\n",
    "load_dotenv()\n",
    "open_ai_api_key = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GraphQL Schema Parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# with open('../assets/schemas/opensea/opensea_original_schema.graphql', 'r') as schema_file:\n",
    "#     schema_str = schema_file.read()\n",
    "\n",
    "# # TEMP: Try to parse the schema given the errors we are seeing\n",
    "# custom_definitions = '''\n",
    "# scalar BigDecimal\n",
    "# scalar BigInt\n",
    "\n",
    "# directive @entity on OBJECT\n",
    "# directive @dailySnapshot on OBJECT\n",
    "# directive @regularPolling on OBJECT\n",
    "# directive @derivedFrom on OBJECT\n",
    "# directive @transaction on OBJECT\n",
    "# '''\n",
    "\n",
    "# full_schema_str = custom_definitions + schema_str\n",
    "# schema = build_schema(full_schema_str)\n",
    "\n",
    "# for type_name, graphql_type in schema.type_map.items():\n",
    "#     if type_name.startswith('__'):\n",
    "#         continue  # Skip introspection types\n",
    "#     print(f\"Type: {type_name}\")\n",
    "#     for field_name, field in graphql_type.fields.items():\n",
    "#         print(f\"  Field: {field_name} (type: {field.type})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LangGraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import getpass\n",
    "# import os\n",
    "\n",
    "# if \"OPENAI_API_KEY\" not in os.environ:\n",
    "#     os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter your OpenAI API key: \")\n",
    "# open_ai_api_key = os.environ[\"OPENAI_API_KEY\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from typing import Annotated\n",
    "\n",
    "# from langchain_openai import OpenAI\n",
    "# from typing_extensions import TypedDict\n",
    "\n",
    "# from langgraph.graph import StateGraph\n",
    "# from langgraph.graph.message import add_messages\n",
    "\n",
    "\n",
    "# class State(TypedDict):\n",
    "#     messages: Annotated[list, add_messages]\n",
    "\n",
    "\n",
    "# graph_builder = StateGraph(State)\n",
    "\n",
    "\n",
    "# llm = OpenAI(model=\"gpt-4\")\n",
    "\n",
    "\n",
    "# def chatbot(state: State):\n",
    "#     return {\"messages\": [llm.invoke(state[\"messages\"])]}\n",
    "\n",
    "# graph_builder.add_node(\"chatbot\", chatbot)\n",
    "# graph_builder.set_entry_point(\"chatbot\")\n",
    "# graph_builder.set_finish_point(\"chatbot\")\n",
    "# graph = graph_builder.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from IPython.display import Image, display\n",
    "\n",
    "# try:\n",
    "#     display(Image(graph.get_graph().draw_mermaid_png()))\n",
    "# except Exception:\n",
    "#     pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OpenAI Directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! I'm here and ready to help. How can I assist you today?\n"
     ]
    }
   ],
   "source": [
    "client = OpenAI(\n",
    "    api_key=os.environ.get(\"OPENAI_API_KEY\"),  # This is the default and can be omitted\n",
    ")\n",
    "\n",
    "def gpt_chat_completion(client, message, model=\"gpt-4o\"):\n",
    "    return client.chat.completions.create(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": message,\n",
    "        }\n",
    "    ],\n",
    "    model=model,\n",
    ")\n",
    "\n",
    "print(gpt_chat_completion(client, \"Hello! How are you?\").choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial Test Set \n",
    "\n",
    "### Prompts \n",
    "\n",
    "- [ ] One entity quality vs. another entity quality \n",
    "- [ ] One column quality vs. another column quality \n",
    "- [ ] One schema quality vs. another schema quality \n",
    "\n",
    "### Demonstrations \n",
    "\n",
    "- [ ] A high quality entity (description)\n",
    "- [ ] A high quality column (description)\n",
    "- [ ] A high quality schema (description)\n",
    "\n",
    "### Quality Categories\n",
    "\n",
    "- [ ] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"tests/assets/entity_comparison_assets.json\", \"r\") as f:\n",
    "    entity_comparison_assets = json.load(f)\n",
    "\n",
    "gold_entity_comparison = \"\".join(entity_comparison_assets[\"gold\"][\"prompt\"])\n",
    "four_entity_comparison = \"\".join(entity_comparison_assets[\"four\"][\"prompt\"])\n",
    "three_entity_comparison = \"\".join(entity_comparison_assets[\"three\"][\"prompt\"])\n",
    "two_entity_comparison = \"\".join(entity_comparison_assets[\"two\"][\"prompt\"])\n",
    "one_entity_comparison = \"\".join(entity_comparison_assets[\"one\"][\"prompt\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Template 'entity_comparison_prompt.txt'>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expecting output: 3 \n",
      "\n",
      "{'reasoning': \"First, we must compare the EVALUATION DOCUMENTATION with the GOLD DOCUMENTATION. Upon careful examination, both documentations are identical in their format and content. Each column in both documentations contains a full English sentence that clearly describes the meaning of the column, eliminating ambiguity. For example, all entries specify the context when necessary, such as 'The marketplace that this snapshot belongs to' rather than a less informative 'Market.' Also, they include concise yet comprehensive descriptions, such as explaining that a column represents 'Cumulative trade volume (in ETH).' The queries are syntactically correct, using proper capitalization and punctuation. Therefore, the provided documentation matches the GOLD documentation criteria and lacks any missing information, misleading descriptions, or grammar errors.\", 'correctness': 3}\n",
      "\n",
      " Expecting output: 4 \n",
      "\n",
      "{'reasoning': \"The evaluation documentation provides detailed explanations for each field in the 'MarketplaceDailySnapshot' table schema. This includes comprehensive descriptions of each column which are more detailed than the gold documentation. For example, the 'id' field adds context by specifying the structure: '{ Contract address }-{# of days since Unix epoch time}', while the gold documentation lacks this detail. Additionally, the evaluation documentation includes explanations of the data type formatting such as 'in Ether, formatted as a decimal', which increases clarity. These elaborations ensure that the documentation exceeds the gold standard by offering more comprehensive context and detail. Furthermore, the descriptions adhere to proper grammatical structuring with complete sentences, making them clear and easy to understand.\", 'correctness': 4}\n",
      "\n",
      " Expecting output: 3 \n",
      "\n",
      "{'reasoning': 'To evaluate the quality of the documentation provided in the EVALUATION DOCUMENTATION, we must compare it against the GOLD DOCUMENTATION using the specified correctness criterion. \\n\\n1. **Correctness and Completeness**: Both the EVALUATION DOCUMENTATION and GOLD DOCUMENTATION are provided in the form of type definitions with comments explaining each field. The two documents are identical line by line. Each field in the schema is clearly defined with accurate and descriptive comments that explain its meaning and purpose. This makes the documentation complete and free from ambiguity. As a result, the documentation clearly explains what each field represents. \\n\\n2. **Grammar and Clarity**: Both documents follow proper grammatical rules with correct capitalization and punctuation. The use of full English sentences is consistent throughout the documentation. This enhances readability and clarity, meeting the criterion for Perfect documentation.\\n\\n3. **Comparison with GOLD**: Since the EVALUATION DOCUMENTATION matches the GOLD DOCUMENTATION exactly and all step-by-step checks confirm that the comments are clear, complete, and correctly formatted, it meets the criteria for Perfect documentation.\\n\\nBased on this detailed analysis, the provided EVALUATION DOCUMENTATION not only matches the GOLD DOCUMENTATION but meets all the additional criteria required for Perfect column descriptions. Therefore, its correctness rating should be 4.', 'correctness': 4}\n",
      "\n",
      " Expecting output: 2 \n",
      "\n",
      "{'reasoning': \"To determine the correctness of the EVALUATION DOCUMENTATION as compared to the GOLD DOCUMENTATION, we need to evaluate it based on the criteria provided. \\n\\n1. Correctness:\\n   - The ID field: Both descriptions denote that it is of type ID and indicate it relates to {Contract address}-{# of days since epoch time}, although the GOLD DOCUMENTATION specifically includes 'Unix'. This is a minor detail, thus the EVALUATION DOCUMENTATION is still clear.\\n   - The marketplace field: The GOLD DOCUMENTATION specifies 'The marketplace that this snapshot belongs to,' whereas the EVALUATION simply states 'The marketplace.' The GOLD version offers a bit more context, making it clearer, but the EVALUATION's isn't incorrect.\\n   - Block number and timestamp fields: Both documentations describe these adequately, with only slight differences in wording that don't introduce any ambiguity.\\n   - The collectionCount, cumulativeTradeVolumeETH, tradeCount, cumulativeUniqueTraders, dailyActiveTraders, dailyTradedCollectionCount, and dailyTradedItemCount fields: The descriptions are similar, with the GOLD offering more precise or slightly more informative descriptions. However, these differences are minor and don't create confusion.\\n   - The marketplaceRevenueETH, creatorRevenueETH, and totalRevenueETH fields: The GOLD DOCUMENTATION includes clarifications (e.g., 'aka protocol fee'), which could enhance understanding, but the absence of these doesn't make the EVALUATION version incorrect.\\n\\n2. Ambiguity and Information:\\n   - The EVALUATION DOCUMENTATION generally lacks the additional context provided in the GOLD DOCUMENTATION. The GOLD tends to slightly expand on certain fields, such as clarifying what 'revenue' or 'trade count' pertains to. However, none of this absence in EVALUATION leads to incorrect or misleading information.\\n\\nOverall, while the EVALUATION DOCUMENTATION is less detailed than the GOLD DOCUMENTATION, it doesn't present any incorrect information; rather, it lacks some specificity and contextual depth provided in the GOLD.\\n\\nTherefore, the EVALUATION DOCUMENTATION can be rated as 'Almost Perfect' as it closely matches the GOLD DOCUMENTATION, although not to the level of detail that would be perfect documentation.\", 'correctness': 3}\n",
      "\n",
      " Expecting output: 1 \n",
      "\n",
      "{'reasoning': \"The evaluation documentation and the gold documentation are provided for a table schema, and the goal is to evaluate the correctness of the evaluation documentation against the gold documentation using the provided criteria. Let's assess each component:\\n\\n1. **id**: The evaluation documentation lacks a detailed description, while the gold documentation includes context about the composition of the ID, mentioning it combines 'Contract address' and '# of days since Unix epoch time'. This additional information provides clarity that is missing in the evaluation documentation.\\n\\n2. **marketplace**: The evaluation describes it merely as 'marketplace', whereas the gold documentation provides complete context 'The marketplace that this snapshot belongs to'. The gold documentation offers better clarity about the reference.\\n\\n3. **blockNumber**: The gold documentation provides a specific context that the block number is 'where the snapshot is taken', which helps in understanding the temporal aspect of when the block is recorded. This is absent in the evaluation documentation.\\n\\n4. **timestamp**: Similar to the blockNumber, the gold documentation describes it as 'Block timestamp when the snapshot is taken', specifying the importance of the timestamp in relation to the snapshot event, which the evaluation documentation fails to include.\\n\\n5. **collectionCount**: The evaluation documentation is vague with 'collections on marketplace', while the gold explicitly states 'Number of collections listed on the marketplace', which provides a complete description.\\n\\n6. **cumulativeTradeVolumeETH**: The evaluation documentation simply states 'trade volume', whereas the gold version clarifies it as 'Cumulative trade volume (in ETH)', specifying the measurement unit and cumulative nature.\\n\\n7. **marketplaceRevenueETH**: The evaluation provides 'creator fee', which seems to be incorrect and misleading. The gold documentation gives 'Revenue that goes to the marketplace protocol, aka protocol fee', clarifying the meaning and aligning with the column name.\\n\\n8. **creatorRevenueETH**: Similarly, the evaluation gives 'marketplace fee', which is misleading, while the gold documentation states 'Revenue that goes to creator, aka royalty fee', providing clarity and correcting the misinterpretation.\\n\\n9. **totalRevenueETH**: The gold documentation explains it as the 'Sum of marketplaceRevenueETH and creatorRevenueETH', giving a complete understanding that the evaluation documentation lacks.\\n\\n10. **tradeCount**: The evaluation's 'trade count of traders' is misleading. The gold documentation clarifies it as the 'Trade count of all collections on the marketplace', providing accurate context and target scope.\\n\\n11. **cumulativeUniqueTraders**: The evaluation simply names it without extra context. However, the gold mentions it straightforwardly 'Cumulative unique traders', which are equivalent but the gold standard provides clearer description grammar.\\n\\n12. **dailyActiveTraders**: Both the evaluation and the gold documentation provide similar descriptions. However, minor issues in capitalization exist in the evaluation version.\\n\\n13. **dailyTradedCollectionCount**: The gold specifies 'Number of traded collections of the day', while the evaluation leaves out 'Number' leading to a slightly incomplete understanding.\\n\\n14. **dailyTradedItemCount**: Similar to the point above, the evaluation misses the clarity that gold provides by stating 'Number of traded items of the day'.\\n\\nOverall, the evaluation documentation is lacking in clarity, context, and detail compared to the gold documentation, making it a somewhat correct representation due to missing and sometimes misleading descriptions.\", 'correctness': 2}\n"
     ]
    }
   ],
   "source": [
    "# Function to reload the template\n",
    "def reload_template(template_path=\"../assets/prompts\", template_name=\"entity_comparison_prompt.txt\"):\n",
    "    env = Environment(loader=FileSystemLoader(template_path))\n",
    "    env.cache.clear()\n",
    "    entity_comparison_prompt_template = env.get_template(template_name)\n",
    "    return entity_comparison_prompt_template\n",
    "\n",
    "def parse_response(response): \n",
    "    response = response.choices[0].message.content\n",
    "    response = response.strip('```json').strip('```').strip()\n",
    "    response = json.loads(response)\n",
    "    return response\n",
    "\n",
    "example_prompt = reload_template()\n",
    "example_prompt_filled = example_prompt.render({\"entity_pred\": \"<example prediction goes here>\", \"entity_gold\": \"<example gold goes here>\"})\n",
    "\n",
    "entity_comparison_prompt_template = reload_template()\n",
    "gold_output = entity_comparison_prompt_template.render({\"entity_pred\": {gold_entity_comparison}, \"entity_gold\": {gold_entity_comparison}})\n",
    "four_output = entity_comparison_prompt_template.render({\"entity_pred\": {four_entity_comparison}, \"entity_gold\": {gold_entity_comparison}})\n",
    "three_output = entity_comparison_prompt_template.render({\"entity_pred\": {three_entity_comparison}, \"entity_gold\": {gold_entity_comparison}})\n",
    "two_output = entity_comparison_prompt_template.render({\"entity_pred\": {two_entity_comparison}, \"entity_gold\": {gold_entity_comparison}})\n",
    "one_output = entity_comparison_prompt_template.render({\"entity_pred\": {one_entity_comparison}, \"entity_gold\": {gold_entity_comparison}})\n",
    "\n",
    "print(\"Expecting output: 3 \\n\")\n",
    "gold_comparison = parse_response(gpt_chat_completion(client, gold_output))\n",
    "print(gold_comparison)\n",
    "\n",
    "print(\"\\n Expecting output: 4 \\n\")\n",
    "four_comparison = parse_response(gpt_chat_completion(client, four_output))\n",
    "print(four_comparison)\n",
    "\n",
    "print(\"\\n Expecting output: 3 \\n\")\n",
    "three_comparison = parse_response(gpt_chat_completion(client, three_output))\n",
    "print(three_comparison)\n",
    "\n",
    "print(\"\\n Expecting output: 2 \\n\")\n",
    "two_comparison = parse_response(gpt_chat_completion(client, two_output))\n",
    "print(two_comparison)\n",
    "\n",
    "print(\"\\n Expecting output: 1 \\n\")\n",
    "one_comparison = parse_response(gpt_chat_completion(client, one_output))\n",
    "print(one_comparison)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_comparison_prompt_template = reload_template(\n",
    "    template_path=\"../assets/prompts\",\n",
    "    template_name=\"entity_comparison_revision.txt\",\n",
    ")\n",
    "\n",
    "entity_comparison_prompt_modification = entity_comparison_prompt_template.render(\n",
    "    {\n",
    "        \"original_prompt\": {example_prompt_filled}, \n",
    "\n",
    "        \"four_result_correct\": {\"True\" if four_comparison[\"correctness\"] == 4 else \"False\"},\n",
    "        \"four_result\": {four_comparison[\"reasoning\"]},\n",
    "        \n",
    "        \"three_result_correct\": {\"True\" if three_comparison[\"correctness\"] == 3 else \"False\"},\n",
    "        \"three_result\": {three_comparison[\"reasoning\"]},\n",
    "        \n",
    "        \"two_result_correct\": {\"True\" if two_comparison[\"correctness\"] == 2 else \"False\"},\n",
    "        \"two_result\": {two_comparison[\"reasoning\"]},\n",
    "        \n",
    "        \"one_result_correct\": {\"True\" if one_comparison[\"correctness\"] == 1 else \"False\"},\n",
    "        \"one_result\": {one_comparison[\"reasoning\"]},\n",
    "    }\n",
    ")\n",
    "\n",
    "prompt_revision = gpt_chat_completion(client, entity_comparison_prompt_modification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"../assets/prompts/entity_comparison_prompt_revised.txt\"\n",
    "\n",
    "prompt_revision = parse_response(prompt_revision)\n",
    "revised_prompt = prompt_revision[\"modified_prompt\"]\n",
    "\n",
    "# Replace the placeholder with Jinja syntax\n",
    "revised_prompt = revised_prompt.replace(r\"{entity_pred}\", r\"{{ entity_pred }}\")\n",
    "revised_prompt = revised_prompt.replace(r\"{entity_gold}\", r\"{{ entity_gold }}\")\n",
    "\n",
    "with open(file_path, \"w\") as file:\n",
    "    file.write(revised_prompt)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "graphdoc-x8ppHhEw-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
