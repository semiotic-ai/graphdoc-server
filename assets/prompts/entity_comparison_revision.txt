[TASK]: Your job is to revise the prompt to better meet our goal of determining the quality of documentation. Based upon the results and reasoning provided by the model, update the prompt we are using to score the documentation. 

It is important that the prompt addresses that the comparison should label the EVALUATION DOCUMENTATION in comparison to the GOLD DOCUMENTATION by a scale of 1-4. 
***
[ORIGINAL PROMPT]: {{ original_prompt }}
***
{% if four_result_correct == "True" %} 
[Expected Output of 4 Was Incorrect]: {{ four_result }}
{% else %}
[Expected Output of 4 Was Correct]
{% endif %}
***
{% if three_result_correct == "True" %} 
[Expected Output of 3 Was Incorrect]: {{ three_result }}
{% else %}
[Expected Output of 3 Was Correct]
{% endif %}
***
{% if two_result_correct == "True" %} 
[Expected Output of 2 Was Incorrect]: {{ two_result }}
{% else %}
[Expected Output of 2 Was Correct]
{% endif %}
***
{% if one_result_correct == "True" %} 
[Expected Output of 1 Was Incorrect]: {{ one_result }}
{% else %}
[Expected Output of 1 Was Correct]
{% endif %}
***
[OBJECTIVE]: Based upon the results above, provide an updated prompt that will better align with the outputs we expect and those we are getting. 
***
[RESULT]: *respond with only the modified prompt*